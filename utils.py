'''
@authors: Andrey Prokpenko (e-mail: prokopenkoav@ornl.gov)
        : Debangshu Mukherjee (e-mail: mukherjeed@ornl.gov)
        : Massimiliano Lupo Pasini (e-mail: lupopasinim@ornl.gov)
        : Nouamane Laanait (e-mail: laanaitn@ornl.gov)
        : Simona Perotto (e-mail: simona.perotto@polimi.it)
        : Vitaliy Starchenko  (e-mail: starchenkov@ornl.gov)
        : Vittorio Gabbi (e-mail: vittorio.gabbi@mail.polimi.it) 
'''
##########################################
import os
import numpy as np
from matplotlib import pyplot as plt
import torch
from torch import autograd
from torch import nn
from torch.autograd.variable import Variable
# from mpi4py import MPI

if torch.cuda.is_available():
    import pycuda
    from pycuda import compiler
    import pycuda.driver as drv

    drv.init()


################################################################


def ones_target(size):
    '''
    Tensor containing ones, with shape = size
    '''
    data = Variable(torch.ones(size, 1))
    return data


def ones_target_resnet(size):
    '''
    Tensor containing ones, with shape = size
    '''
    data = Variable(torch.ones(size))
    return data


def ones_target_smooth(size):
    '''
    Tensor containing 0.9s, with shape = size
    '''
    data = torch.full((size,), 0.9)
    return data


def zeros_target(size):
    '''
    Tensor containing zeros, with shape = size
    '''
    data = Variable(torch.zeros(size, 1))
    return data


def zeros_target_resnet(size):
    '''
    Tensor containing zeros, with shape = size
    '''
    data = Variable(torch.zeros(size))
    return data


def zeros_target_smooth(size):
    '''
    Tensor containing zeros, with shape = size
    '''
    data = torch.full((size,), 0.1)

    return data


def noise(size, noise_size):
    '''
    Generates a 1-d vector of gaussian sampled random values
    '''
    n = Variable(torch.randn(size, noise_size))
    return n


def images_to_vectors(images):
    image_dim = images.size(1) * images.size(2) * images.size(3)
    return images.view(images.size(0), image_dim)


def vectors_to_images(vectors, array_dim):
    return vectors.view(
        vectors.size(0), array_dim[0], array_dim[1], array_dim[2]
    )


def images_to_vectors_cifar10(images):
    return images.view(images.size(0), 3072)


def vectors_to_images_cifar10(vectors):
    return vectors.view(vectors.size(0), 3, 32, 32)


def count_gpus():
    number = 0
    if torch.cuda.is_available():
        number = torch.cuda.device_count()
        print(number, " - GPUs found")
    else:
        print(number, " - GPU NOT found")
    return number


def get_gpus_list():
    gpu_list = []
    if torch.cuda.is_available():
        # print("%d device(s) found." % drv.Device.count())
        for ordinal in range(drv.Device.count()):
            dev = drv.Device(ordinal)
            # print (ordinal, dev.name())
            gpu_list.append(ordinal)
    return gpu_list


def get_gpu(number):
    gpus_list = get_gpus_list()
    if torch.cuda.is_available():
        if number not in gpus_list:
            raise ValueError(
                'The GPU ID:'
                + str(number)
                + ' is not inside the list of GPUs available'
            )
        else:
            torch.cuda.device_count()
            device = torch.device(
                "cuda:" + str(number)
            )  # you can continue going on here, like cuda:1 cuda:2....etc.
            print(
                "MPI rank "
                + str(int(os.environ.get('LOCAL_RANK', 0)))
                + " running on the GPU with ID: "
                + str(number)
            )
    else:
        device = torch.device("cpu")
        print(
            "MPI rank "
            + str(MPI.COMM_WORLD.Get_rank())
            + " running on the CPU - GPU is NOT available"
        )
    return device


#############################################################################


def zero_grad(params):
    for p in params:
        if p.grad is not None:
            p.grad.detach()
            p.grad.zero_()


def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)


def Hvp_vec(grad_vec, params, vec, retain_graph=False):
    if torch.isnan(grad_vec).any():
        print('grad vec nan')
        raise ValueError('grad Nan')
    if torch.isnan(vec).any():
        print('vec nan')
        raise ValueError('vec Nan')
    try:
        grad_grad = autograd.grad(
            grad_vec, params, grad_outputs=vec, retain_graph=retain_graph
        )
        hvp = torch.cat([g.contiguous().view(-1) for g in grad_grad])
        if torch.isnan(hvp).any():
            print('hvp nan')
            raise ValueError('hvp Nan')
    except:
        # print('filling zero for None')
        grad_grad = autograd.grad(
            grad_vec,
            params,
            grad_outputs=vec,
            retain_graph=retain_graph,
            allow_unused=True,
        )
        grad_list = []
        for i, p in enumerate(params):
            if grad_grad[i] is None:
                grad_list.append(torch.zeros_like(p))
            else:
                grad_list.append(grad_grad[i].contiguous().view(-1))
        hvp = torch.cat(grad_list)
        if torch.isnan(hvp).any():
            raise ValueError('hvp Nan')
    return hvp


def hessian_vec(grad_vec, var, retain_graph=False):
    v = torch.ones_like(var)
    (vec,) = autograd.grad(
        grad_vec,
        var,
        grad_outputs=v,
        allow_unused=True,
        retain_graph=retain_graph,
    )
    return vec


def binary_cross_entropy(x, y):
    loss = -(x.log() * y + (1 - x).log() * (1 - y))
    return loss.mean()


class Richardson(object):
    def __init__(self, matrix, rhs, tol, maxiter, relaxation, verbose=False):
        """
        :param matrix: coefficient matrix
        :param rhs: right hand side
        :param tol: tolerance for stopping criterion based on the relative residual
        :param maxiter: maximum number of iterations
        :param relaxation: relaxation parameter for Richardson
        :param initial_guess: initial guess
        :return: matrix ** -1 * rhs
        """

        self.rhs = rhs
        self.matrix = matrix
        self.tol = tol
        self.maxiter = maxiter
        self.relaxation = relaxation
        self.rhs_norm = torch.norm(rhs, 2)
        self.iteration_count = 0
        self.verbose = verbose

    def print_verbose(self, *args, **kwargs):
        if self.verbose:
            print(*args, **kwargs)

    def solve(self, initial_guess):
        ## TODO: consider passing initial guess to solve()

        residual = self.rhs - self.matrix @ initial_guess
        residual_norm = residual.norm()
        relative_residual_norm = residual_norm / self.rhs_norm

        solution = initial_guess

        while (
            relative_residual_norm > self.tol
            and self.iteration_count < self.maxiter
        ):
            ## TODO: consider making all of these non-attributes and just return them
            solution = solution + self.relaxation * residual

            residual = self.rhs - torch.matmul(self.matrix, solution)
            residual_norm = residual.norm()
            relative_residual_norm = residual_norm / self.rhs_norm
            self.iteration_count += 1
            self.print_verbose(
                "Richardson converged in ",
                str(self.iteration_count),
                " iteration with relative residual norm: ",
                str(relative_residual_norm),
                end='...',
            )

        # Do not return because it's already an attribute
        return solution


def general_conjugate_gradient(
    grad_x,
    grad_y,
    x_params,
    y_params,
    kk,
    lr_x,
    lr_y,
    x=None,
    nsteps=10,
    residual_tol=1e-16,
    device_x=torch.device('cpu'),
    device_y=torch.device('cpu'),
):
    '''

    :param grad_x:
    :param grad_y:
    :param x_params:
    :param y_params:
    :param b:
    :param lr_x:
    :param lr_y:
    :param x:
    :param nsteps:
    :param residual_tol:
    :param device:
    :return: (I + sqrt(lr_x) * D_xy * lr_y * D_yx * sqrt(lr_x)) ** -1 * b

    '''
    if x is None:
        x = torch.zeros(kk.shape[0], device=device_x)
    if grad_x.shape != kk.shape:
        raise RuntimeError('CG: hessian vector product shape mismatch')
    lr_x = lr_x.sqrt().to(device_x)
    lr_y = lr_y.to(device_y)

    mm = kk.clone().detach()
    mm = mm.to(device_x)
    jj = mm.clone().detach()
    jj = jj.to(device_x)
    rdotr = torch.dot(mm, mm)
    residual_tol = residual_tol * rdotr
    x_params = tuple(x_params)
    y_params = tuple(y_params)
    for i in range(nsteps):
        # To compute Avp
        # h_1 = Hvp_vec(grad_vec=grad_x, params=y_params, vec=lr_x * p, retain_graph=True)
        h_1 = Hvp_vec(
            grad_vec=grad_x.to(device_x),
            params=y_params,
            vec=lr_x * jj,
            retain_graph=True,
        ).mul_(lr_y)
        # h_1.mul_(lr_y)
        # lr_y * D_yx * b
        # h_2 = Hvp_vec(grad_vec=grad_y, params=x_params, vec=lr_y * h_1, retain_graph=True)
        h_2 = Hvp_vec(
            grad_vec=grad_y.to(device_x),
            params=x_params,
            vec=h_1.to(device_x),
            retain_graph=True,
        ).mul_(lr_x)
        # h_2.mul_(lr_x)
        # lr_x * D_xy * lr_y * D_yx * b
        Avp_ = jj + h_2

        alpha = rdotr / torch.dot(jj, Avp_)
        x.data.add_(alpha * jj)
        mm.data.add_(-alpha * Avp_)
        new_rdotr = torch.dot(mm, mm)
        beta = new_rdotr / rdotr
        jj = mm + beta * jj
        rdotr = new_rdotr
        if rdotr < residual_tol:
            break
    return x, i + 1


#######################################################################
def general_conjugate_gradient_jacobi(
    grad_x,
    x_params,
    right_side,
    x=None,
    nsteps=10,
    residual_tol=1e-16,
    device=torch.device('cpu'),
):
    '''

    :param grad_x:
    :param x_params:
    :param b:
    :param lr_x:
    :param x:
    :param nsteps:
    :param residual_tol:
    :param device:
    :return: (A) ** -1 * (right_side)

    '''
    if x is None:
        x = torch.zeros(right_side.shape[0], device=device)
    else:
        x = x.to(device)

    right_side_clone1 = right_side.clone().detach()
    right_side_clone2 = right_side.clone().detach()
    right_side_clone1 = right_side_clone1.to(device)
    right_side_clone2 = right_side_clone2.to(device)

    rdotr = torch.dot(right_side_clone1, right_side_clone1)
    residual_tol = residual_tol * rdotr
    x_params = tuple(x_params)

    for i in range(nsteps):
        h_1 = Hvp_vec(
            grad_vec=grad_x.to(device),
            params=x_params,
            vec=2 * x,
            retain_graph=True,
        )
        H = -h_1.to(device) + x
        Avp_ = right_side_clone2 + H

        alpha = rdotr / torch.dot(right_side_clone2, Avp_)
        x.data.add_(alpha * right_side_clone2)
        right_side_clone1.data.add_(-alpha * Avp_)
        new_rdotr = torch.dot(right_side_clone1, right_side_clone1.to(device))
        beta = new_rdotr / rdotr
        right_side_clone2 = right_side_clone1 + beta * right_side_clone2
        rdotr = new_rdotr
        if rdotr < residual_tol:
            break
    return x, i + 1


###########################################
